{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import collections\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from typing import Iterator\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"A pytorch-like Dataset class.\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ArrayDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "    \n",
    "\n",
    "class Text2TextDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "\n",
    "\n",
    "class CausalLMDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "    \n",
    "\n",
    "class ConditionalTextDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "    \n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "\n",
    "\n",
    "class ImageToImageDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "    \n",
    "\n",
    "class ConditionalTextDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "    \n",
    "\n",
    "class ImageToTextDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "    \n",
    "\n",
    "class TextToImageDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.array\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Dataloder in Vanilla Jax\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: Dataset, \n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        drop_last: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.keys = PRNGSequence(seed=Config.default().global_seed)\n",
    "        self.data_len = len(dataset)  # Length of the dataset\n",
    "        self.indices = jnp.arange(self.data_len) # available indices in the dataset\n",
    "        self.pose = 0  # record the current position in the dataset\n",
    "        self._shuffle()\n",
    "\n",
    "    def _shuffle(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = jax.random.permutation(next(self.keys), self.indices)\n",
    "        \n",
    "    def _stop_iteration(self):\n",
    "        self.pose = 0\n",
    "        self._shuffle()\n",
    "        raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            batches = len(self.dataset) // self.batch_size  # get the floor of division\n",
    "        else:\n",
    "            batches = -(len(self.dataset) // -self.batch_size)  # get the ceil of division\n",
    "        return batches\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pose + self.batch_size <= self.data_len:\n",
    "            batch_indices = self.indices[self.pose: self.pose + self.batch_size]\n",
    "            batch_data = self.dataset[batch_indices]\n",
    "            self.pose += self.batch_size\n",
    "            return batch_data\n",
    "        elif self.pose < self.data_len and not self.drop_last:\n",
    "            batch_indices = self.indices[self.pose:]\n",
    "            batch_data = self.dataset[batch_indices]\n",
    "            self.pose += self.batch_size\n",
    "            return batch_data\n",
    "        else:\n",
    "            self._stop_iteration()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Global configuration for the library\"\"\"\n",
    "    rng_reserve_size: int\n",
    "    global_seed: int\n",
    "\n",
    "    @classmethod\n",
    "    def default(cls):\n",
    "        return cls(rng_reserve_size=1, global_seed=42)\n",
    "\n",
    "class PRNGSequence(Iterator[jax.random.PRNGKey]):\n",
    "    \"\"\"An Interator of Jax PRNGKey (minimal version of `haiku.PRNGSequence`).\"\"\"\n",
    "\n",
    "    def __init__(self, seed: int):\n",
    "        self._key = jax.random.PRNGKey(seed)\n",
    "        self._subkeys = collections.deque()\n",
    "\n",
    "    def reserve(self, num):\n",
    "        \"\"\"Splits additional ``num`` keys for later use.\"\"\"\n",
    "        if num > 0:\n",
    "            new_keys = tuple(jax.random.split(self._key, num + 1))\n",
    "            self._key = new_keys[0]\n",
    "            self._subkeys.extend(new_keys[1:])\n",
    "            \n",
    "    def __next__(self):\n",
    "        if not self._subkeys:\n",
    "            self.reserve(Config.default().rng_reserve_size)\n",
    "        return self._subkeys.popleft()\n",
    "    \n",
    "\n",
    "dataset = ArrayDataset(jnp.ones((1001,256,256)), jnp.ones((1001,256,256)))\n",
    "dataloader = DataLoader(dataset, batch_size= 10, shuffle= True, drop_last= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import pickle\n",
    "import jax.numpy as jnp\n",
    "from flax.training import train_state\n",
    "\n",
    "\n",
    "class DataParallelTrainer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 input_shape,\n",
    "                 train_step,\n",
    "                 optax_optimizer,\n",
    "                 learning_rate,\n",
    "                 weights_filename):\n",
    "        \n",
    "        self.model = model\n",
    "        self.num_parameters = None\n",
    "        self.optimizer = optax_optimizer\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.weights_filename = weights_filename\n",
    "        self.num_devices = jax.local_device_count()\n",
    "        self.train_step = jax.pmap(train_step, axis_name='devices')\n",
    "        self.state = self.create_train_state(learning_rate, input_shape)\n",
    "    \n",
    "\n",
    "    def create_train_state(self, learning_rate, input_shape):\n",
    "        rng = jax.random.PRNGKey(0)\n",
    "        params = self.model.init(rng, jnp.ones(input_shape))['params']\n",
    "        self.num_parameters = sum(param.size for param in jax.tree_util.tree_leaves(params))\n",
    "        state = train_state.TrainState.create(apply_fn=self.model.apply, \n",
    "                                              params=params, \n",
    "                                              tx=self.optimizer(learning_rate))\n",
    "        return jax.device_put_replicated(state, jax.local_devices())\n",
    "    \n",
    "\n",
    "    def train(self, train_loader, num_epochs, val_loader=None):\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for inputs, targets in train_loader:\n",
    "                batch_size = inputs.shape[0]\n",
    "                batch_size_per_device = batch_size // self.num_devices\n",
    "                inputs = inputs.reshape((self.num_devices, batch_size_per_device, -1))\n",
    "                targets = targets.reshape((self.num_devices, batch_size_per_device, -1))\n",
    "                self.state, loss = self.train_step(self.state, {'inputs': inputs, 'targets': targets})\n",
    "                total_loss += jnp.mean(loss)\n",
    "            \n",
    "            mean_loss = total_loss / num_epochs\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {mean_loss}')\n",
    "\n",
    "        if val_loader is not None:\n",
    "            self.validate(val_loader, epoch, num_epochs)\n",
    "        return \n",
    "    \n",
    "\n",
    "    def validate(self, val_loader, epoch, num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in val_loader:\n",
    "            batch_size = inputs.shape[0]\n",
    "            batch_size_per_device = batch_size // self.num_devices\n",
    "            inputs = inputs.reshape((self.num_devices, batch_size_per_device, -1))\n",
    "            targets = targets.reshape((self.num_devices, batch_size_per_device, -1))\n",
    "            _, loss = self.train_step(self.state, {'inputs': inputs, 'targets': targets})\n",
    "            total_loss += jnp.mean(loss)\n",
    "        \n",
    "        mean_loss = total_loss / num_epochs\n",
    "        print(f'Epoch {epoch+1}, Val Loss: {mean_loss}')\n",
    "        if mean_loss < self.best_val_loss:\n",
    "            self.best_val_loss = mean_loss\n",
    "        print(\"New best validation score achieved, saving model...\")\n",
    "        self.save_params()\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def save_params(self):\n",
    "        with open(self.weights_filename, 'wb') as f:\n",
    "            pickle.dump(self.state.params, f)\n",
    "        return\n",
    "\n",
    " \n",
    "    def load_params(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn import softmax\n",
    "\n",
    "def mse_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        predictions = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        loss = jnp.mean(jnp.square(predictions - batch['targets']))\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def binary_cross_entropy_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        loss = -jnp.mean(batch['targets'] * jax.nn.log_sigmoid(logits) +\n",
    "                         (1 - batch['targets']) * jax.nn.log_sigmoid(-logits))\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def categorical_cross_entropy_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        loss = -jnp.mean(jnp.sum(jax.nn.log_softmax(logits) * batch['targets'], axis=-1))\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def sparse_categorical_cross_entropy_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        loss = jax.nn.sparse_softmax_cross_entropy(logits, batch['targets'])\n",
    "        return jnp.mean(loss)\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def cross_entropy_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        loss = -jnp.mean(jnp.sum(softmax(logits) * batch['targets'], axis=-1))\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def l1_regularization_train_step(state, batch, l1_penalty):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        loss = -jnp.mean(jnp.sum(softmax(logits) * batch['targets'], axis=-1))\n",
    "        l1_loss = l1_penalty * jnp.sum(jnp.abs(params))\n",
    "        return loss + l1_loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def l2_regularization_train_step(state, batch, l2_penalty):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        loss = -jnp.mean(jnp.sum(softmax(logits) * batch['targets'], axis=-1))\n",
    "        l2_loss = l2_penalty * jnp.sum(jnp.square(params))\n",
    "        return loss + l2_loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def hinge_loss_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        targets = 2 * batch['targets'] - 1  # Convert {0, 1} labels to {-1, 1}\n",
    "        loss = jnp.mean(jnp.maximum(0, 1 - targets * logits))\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def triplet_loss_train_step(state, batch, margin):\n",
    "    def loss_fn(params):\n",
    "        anchor, positive, negative = batch['anchor'], batch['positive'], batch['negative']\n",
    "        anchor_logits = state.apply_fn({'params': params}, anchor)\n",
    "        positive_logits = state.apply_fn({'params': params}, positive)\n",
    "        negative_logits = state.apply_fn({'params': params}, negative)\n",
    "        \n",
    "        positive_distance = jnp.sum(jnp.square(anchor_logits - positive_logits), axis=-1)\n",
    "        negative_distance = jnp.sum(jnp.square(anchor_logits - negative_logits), axis=-1)\n",
    "        loss = jnp.mean(jnp.maximum(0, margin + positive_distance - negative_distance))\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def contrastive_loss_train_step(state, batch, margin):\n",
    "    def loss_fn(params):\n",
    "        x1, x2, y = batch['x1'], batch['x2'], batch['y']\n",
    "        logits1 = state.apply_fn({'params': params}, x1)\n",
    "        logits2 = state.apply_fn({'params': params}, x2)\n",
    "        \n",
    "        euclidean_distance = jnp.sqrt(jnp.sum(jnp.square(logits1 - logits2), axis=-1))\n",
    "        loss = y * jnp.square(euclidean_distance) + (1 - y) * jnp.square(jnp.maximum(margin - euclidean_distance, 0))\n",
    "        return jnp.mean(loss)\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def cosine_similarity_loss_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        embeddings1 = state.apply_fn({'params': params}, batch['x1'])\n",
    "        embeddings2 = state.apply_fn({'params': params}, batch['x2'])\n",
    "        # Normalize the embeddings to have unit length\n",
    "        embeddings1 = embeddings1 / jnp.linalg.norm(embeddings1, axis=-1, keepdims=True)\n",
    "        embeddings2 = embeddings2 / jnp.linalg.norm(embeddings2, axis=-1, keepdims=True)\n",
    "        # Cosine similarity as dot product of normalized vectors; we subtract from 1 to get the loss\n",
    "        loss = 1 - jnp.sum(embeddings1 * embeddings2, axis=-1)\n",
    "        return jnp.mean(loss)\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def clip_loss_train_step(state, batch, temperature=0.07):\n",
    "    def loss_fn(params):\n",
    "        # Get image and text features from the CLIP model\n",
    "        image_features, text_features = state.apply_fn({'params': params}, batch['images'], batch['text'])\n",
    "        \n",
    "        # Normalize the features\n",
    "        image_features = image_features / jnp.linalg.norm(image_features, axis=-1, keepdims=True)\n",
    "        text_features = text_features / jnp.linalg.norm(text_features, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Calculate the similarity\n",
    "        similarity = jnp.dot(image_features, text_features.T) / temperature\n",
    "        \n",
    "        # CLIP loss calculation\n",
    "        image_loss = jax.nn.softmax_cross_entropy(similarity, jnp.arange(similarity.shape[0]))\n",
    "        text_loss = jax.nn.softmax_cross_entropy(similarity.T, jnp.arange(similarity.shape[0]))\n",
    "        loss = (image_loss + text_loss) / 2\n",
    "        return jnp.mean(loss)\n",
    "        \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def nli_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        # Obtain the logits for the three classes: entailment, contradiction, neutral\n",
    "        logits = state.apply_fn({'params': params}, batch['premise'], batch['hypothesis'])\n",
    "        # Use softmax cross-entropy as the loss function\n",
    "        loss = jax.nn.softmax_cross_entropy(logits, batch['label'])\n",
    "        return jnp.mean(loss)\n",
    "        \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def simclr_train_step(state, batch, temperature=0.1):\n",
    "    def loss_fn(params):\n",
    "        # Obtain representations for two sets of augmented images\n",
    "        representations = state.apply_fn({'params': params}, batch['augmented_images'])\n",
    "        # Split the representations into two halves, one for each augmentation\n",
    "        h1, h2 = jnp.split(representations, 2, axis=0)\n",
    "        \n",
    "        # Normalize the representations\n",
    "        h1 = h1 / jnp.linalg.norm(h1, axis=-1, keepdims=True)\n",
    "        h2 = h2 / jnp.linalg.norm(h2, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Compute the similarity between all pairs\n",
    "        similarity_matrix = jnp.matmul(h1, h2.T) / temperature\n",
    "        \n",
    "        # The contrastive loss function\n",
    "        batch_size = h1.shape[0]\n",
    "        contrastive_labels = jnp.arange(batch_size)\n",
    "        loss = jax.nn.softmax_cross_entropy(similarity_matrix, contrastive_labels)\n",
    "        return jnp.mean(loss)\n",
    "        \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def vae_train_step(vae_state, batch, optimizer):\n",
    "    def vae_loss_fn(params):\n",
    "        reconstructions, mean, logvar = vae_state.apply_fn({'params': params}, batch['inputs'])\n",
    "        # Reconstruction loss (e.g., binary cross-entropy or MSE)\n",
    "        recon_loss = jnp.mean(jnp.square(reconstructions - batch['inputs']))\n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * jnp.mean(1 + logvar - jnp.square(mean) - jnp.exp(logvar))\n",
    "        # Total loss is the sum of reconstruction loss and KL divergence\n",
    "        return recon_loss + kl_loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(vae_loss_fn)\n",
    "    loss, grads = grad_fn(vae_state.params)\n",
    "    updates, new_opt_state = optimizer.update(grads, vae_state.opt_state)\n",
    "    new_vae_state = vae_state.apply_gradients(grads=updates, opt_state=new_opt_state)\n",
    "    return new_vae_state, loss\n",
    "\n",
    "\n",
    "def dice_loss_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        probs = jax.nn.softmax(logits)\n",
    "        intersection = jnp.sum(probs * batch['targets'])\n",
    "        loss = 1 - (2. * intersection + 1.) / (jnp.sum(probs) + jnp.sum(batch['targets']) + 1.)\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def focal_loss_train_step(state, batch, gamma=2.0, alpha=0.25):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        probs = jax.nn.softmax(logits)\n",
    "        ce_loss = -jnp.sum(softmax(logits) * batch['targets'], axis=-1)\n",
    "        p_t = jnp.where(batch['targets'] == 1, probs, 1 - probs)\n",
    "        loss = jnp.mean(-alpha * jnp.power(1 - p_t, gamma) * ce_loss)\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def kl_divergence_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        q_probs = softmax(logits)\n",
    "        p_probs = batch['targets']\n",
    "        loss = jnp.sum(q_probs * (jnp.log(q_probs) - jnp.log(p_probs)), axis=-1)\n",
    "        return jnp.mean(loss)\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def iou_loss_train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        probs = jax.nn.softmax(logits)\n",
    "        intersection = jnp.sum(probs * batch['targets'], axis=(1,2))\n",
    "        union = jnp.sum(probs + batch['targets'], axis=(1,2)) - intersection\n",
    "        iou = intersection / jnp.maximum(union, 1e-6)\n",
    "        loss = 1 - iou  # IoU loss\n",
    "        return jnp.mean(loss)\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def tversky_loss_train_step(state, batch, alpha=0.5, beta=0.5):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        probs = jax.nn.softmax(logits)\n",
    "        true_pos = jnp.sum(batch['targets'] * probs)\n",
    "        false_neg = jnp.sum(batch['targets'] * (1 - probs))\n",
    "        false_pos = jnp.sum((1 - batch['targets']) * probs)\n",
    "        tversky_index = true_pos / (true_pos + alpha * false_neg + beta * false_pos)\n",
    "        loss = 1 - tversky_index\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def gan_train_step(generator_state, discriminator_state, batch, z_dim, rng_key):\n",
    "    # Sample random noise\n",
    "    z = jax.random.normal(rng_key, (batch['real_images'].shape[0], z_dim))\n",
    "\n",
    "    # Discriminator loss\n",
    "    def discriminator_loss_fn(d_params):\n",
    "        fake_images = generator_state.apply_fn({'params': generator_state.params}, z)\n",
    "        real_logits = discriminator_state.apply_fn({'params': d_params}, batch['real_images'])\n",
    "        fake_logits = discriminator_state.apply_fn({'params': d_params}, fake_images)\n",
    "        real_loss = jax.nn.sigmoid_cross_entropy_with_logits(real_logits, jnp.ones_like(real_logits))\n",
    "        fake_loss = jax.nn.sigmoid_cross_entropy_with_logits(fake_logits, jnp.zeros_like(fake_logits))\n",
    "        return jnp.mean(real_loss + fake_loss)\n",
    "\n",
    "    # Generator loss\n",
    "    def generator_loss_fn(g_params):\n",
    "        fake_images = generator_state.apply_fn({'params': g_params}, z)\n",
    "        fake_logits = discriminator_state.apply_fn({'params': discriminator_state.params}, fake_images)\n",
    "        return jnp.mean(jax.nn.sigmoid_cross_entropy_with_logits(fake_logits, jnp.ones_like(fake_logits)))\n",
    "\n",
    "    # Update discriminator\n",
    "    d_grad_fn = jax.value_and_grad(discriminator_loss_fn)\n",
    "    d_loss, d_grads = d_grad_fn(discriminator_state.params)\n",
    "    discriminator_state = discriminator_state.apply_gradients(grads=d_grads)\n",
    "\n",
    "    # Update generator\n",
    "    g_grad_fn = jax.value_and_grad(generator_loss_fn)\n",
    "    g_loss, g_grads = g_grad_fn(generator_state.params)\n",
    "    generator_state = generator_state.apply_gradients(grads=g_grads)\n",
    "\n",
    "    return generator_state, discriminator_state, g_loss, d_loss\n",
    "\n",
    "\n",
    "def wgan_train_step(generator_state, discriminator_state, batch, z_dim, rng_key):\n",
    "    # Sample random noise\n",
    "    z = jax.random.normal(rng_key, (batch['real_images'].shape[0], z_dim))\n",
    "\n",
    "    # Discriminator (Critic) loss\n",
    "    def discriminator_loss_fn(d_params):\n",
    "        fake_images = generator_state.apply_fn({'params': generator_state.params}, z)\n",
    "        real_logits = discriminator_state.apply_fn({'params': d_params}, batch['real_images'])\n",
    "        fake_logits = discriminator_state.apply_fn({'params': d_params}, fake_images)\n",
    "        return jnp.mean(fake_logits - real_logits)\n",
    "\n",
    "    # Generator loss\n",
    "    def generator_loss_fn(g_params):\n",
    "        fake_images = generator_state.apply_fn({'params': g_params}, z)\n",
    "        fake_logits = discriminator_state.apply_fn({'params': discriminator_state.params}, fake_images)\n",
    "        return -jnp.mean(fake_logits)\n",
    "\n",
    "    # Update discriminator (critic)\n",
    "    d_grad_fn = jax.value_and_grad(discriminator_loss_fn)\n",
    "    d_loss, d_grads = d_grad_fn(discriminator_state.params)\n",
    "    discriminator_state = discriminator_state.apply_gradients(grads=d_grads)\n",
    "\n",
    "    # Update generator\n",
    "    g_grad_fn = jax.value_and_grad(generator_loss_fn)\n",
    "    g_loss, g_grads = g_grad_fn(generator_state.params)\n",
    "    generator_state = generator_state.apply_gradients(grads=g_grads)\n",
    "\n",
    "    return generator_state, discriminator_state, g_loss, d_loss\n",
    "\n",
    "\n",
    "def lsgan_train_step(generator_state, discriminator_state, batch, z_dim, rng_key):\n",
    "    # Sample random noise\n",
    "    z = jax.random.normal(rng_key, (batch['real_images'].shape[0], z_dim))\n",
    "\n",
    "    # Discriminator loss\n",
    "    def discriminator_loss_fn(d_params):\n",
    "        fake_images = generator_state.apply_fn({'params': generator_state.params}, z)\n",
    "        real_logits = discriminator_state.apply_fn({'params': d_params}, batch['real_images'])\n",
    "        fake_logits = discriminator_state.apply_fn({'params': d_params}, fake_images)\n",
    "        real_loss = jnp.square(real_logits - 1)\n",
    "        fake_loss = jnp.square(fake_logits)\n",
    "        return jnp.mean(0.5 * (real_loss + fake_loss))\n",
    "\n",
    "    # Generator loss\n",
    "    def generator_loss_fn(g_params):\n",
    "        fake_images = generator_state.apply_fn({'params': g_params}, z)\n",
    "        fake_logits = discriminator_state.apply_fn({'params': discriminator_state.params}, fake_images)\n",
    "        return jnp.mean(0.5 * jnp.square(fake_logits - 1))\n",
    "\n",
    "    # Update discriminator\n",
    "    d_grad_fn = jax.value_and_grad(discriminator_loss_fn)\n",
    "    d_loss, d_grads = d_grad_fn(discriminator_state.params)\n",
    "    discriminator_state = discriminator_state.apply_gradients(grads=d_grads)\n",
    "\n",
    "    # Update generator\n",
    "    g_grad_fn = jax.value_and_grad(generator_loss_fn)\n",
    "    g_loss, g_grads = g_grad_fn(generator_state.params)\n",
    "    generator_state = generator_state.apply_gradients(grads=g_grads)\n",
    "\n",
    "    return generator_state, discriminator_state, g_loss, d_loss\n",
    "\n",
    "\n",
    "def reinforce_train_step(policy_state, trajectory, optimizer):\n",
    "    def policy_loss_fn(policy_params):\n",
    "        # Extract observations and actions from the trajectory\n",
    "        observations, actions, rewards = trajectory\n",
    "        # Log probabilities of the actions under the policy\n",
    "        log_probs = policy_state.apply_fn({'params': policy_params}, observations)\n",
    "        selected_log_probs = jnp.take_along_axis(log_probs, actions[..., None], axis=-1).squeeze(-1)\n",
    "        # REINFORCE loss\n",
    "        loss = -jnp.mean(jnp.sum(selected_log_probs * rewards, axis=-1))\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(policy_loss_fn)\n",
    "    loss, grads = grad_fn(policy_state.params)\n",
    "    updates, new_opt_state = optimizer.update(grads, policy_state.opt_state)\n",
    "    new_policy_state = policy_state.apply_gradients(grads=updates, opt_state=new_opt_state)\n",
    "    \n",
    "    return new_policy_state, loss\n",
    "\n",
    "\n",
    "def dqn_train_step(q_network_state, batch, optimizer, gamma=0.99):\n",
    "    def q_loss_fn(q_params):\n",
    "        # Extract observations, actions, next observations, and rewards from the batch\n",
    "        observations, actions, next_observations, rewards, dones = batch\n",
    "        # Compute Q values for current observations\n",
    "        q_values = q_network_state.apply_fn({'params': q_params}, observations)\n",
    "        # Select the Q value for the action taken\n",
    "        q_values = jnp.take_along_axis(q_values, actions[..., None], axis=-1).squeeze(-1)\n",
    "        # Compute Q values for next observations\n",
    "        next_q_values = q_network_state.apply_fn({'params': q_params}, next_observations)\n",
    "        # Take max over next Q values for the TD target\n",
    "        max_next_q_values = jnp.max(next_q_values, axis=-1)\n",
    "        # Compute the target Q values\n",
    "        target_q_values = rewards + gamma * max_next_q_values * (1 - dones)\n",
    "        # DQN loss\n",
    "        loss = jnp.mean(jnp.square(q_values - target_q_values))\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(q_loss_fn)\n",
    "    loss, grads = grad_fn(q_network_state.params)\n",
    "    updates, new_opt_state = optimizer.update(grads, q_network_state.opt_state)\n",
    "    new_q_network_state = q_network_state.apply_gradients(grads=updates, opt_state=new_opt_state)\n",
    "    \n",
    "    return new_q_network_state, loss\n",
    "\n",
    "\n",
    "def ppo_train_step(policy_state, value_state, batch, policy_optimizer, value_optimizer, clip_ratio=0.2):\n",
    "    observations, actions, advantages, log_probs_old, returns = batch\n",
    "    \n",
    "    def policy_loss_fn(policy_params):\n",
    "        log_probs = policy_state.apply_fn({'params': policy_params}, observations, actions)\n",
    "        ratio = jnp.exp(log_probs - log_probs_old)\n",
    "        clipped_advantages = jnp.clip(ratio, 1 - clip_ratio, 1 + clip_ratio) * advantages\n",
    "        loss = -jnp.mean(jnp.minimum(ratio * advantages, clipped_advantages))\n",
    "        return loss\n",
    "    \n",
    "    def value_loss_fn(value_params):\n",
    "        values = value_state.apply_fn({'params': value_params}, observations)\n",
    "        loss = jnp.mean(jnp.square(values - returns))\n",
    "        return loss\n",
    "    \n",
    "    # Update policy\n",
    "    policy_grad_fn = jax.value_and_grad(policy_loss_fn)\n",
    "    policy_loss, policy_grads = policy_grad_fn(policy_state.params)\n",
    "    policy_updates, new_policy_opt_state = policy_optimizer.update(policy_grads, policy_state.opt_state)\n",
    "    new_policy_state = policy_state.apply_gradients(grads=policy_updates, opt_state=new_policy_opt_state)\n",
    "    \n",
    "    # Update value function\n",
    "    value_grad_fn = jax.value_and_grad(value_loss_fn)\n",
    "    value_loss, value_grads = value_grad_fn(value_state.params)\n",
    "    value_updates, new_value_opt_state = value_optimizer.update(value_grads, value_state.opt_state)\n",
    "    new_value_state = value_state.apply_gradients(grads=value_updates, opt_state=new_value_opt_state)\n",
    "    return new_policy_state, new_value_state, policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn import softmax\n",
    "from flax import linen as nn\n",
    "from jax.nn.initializers import lecun_normal\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=512, kernel_init=lecun_normal())(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10, kernel_init=lecun_normal())(x)\n",
    "        return x\n",
    "    \n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['inputs'])\n",
    "        loss = -jnp.mean(jnp.sum(softmax(logits) * batch['targets'], axis=-1))\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "trainer = DataParallelTrainer(model=SimpleNN(), \n",
    "                            input_shape=(batch_size_per_device, input_dim),\n",
    "                            train_step=train_step,\n",
    "                            optax_optimizer=optax.adam,\n",
    "                            learning_rate=learning_rate,\n",
    "                            weights_filename=\"params.pkl\")\n",
    "\n",
    "trainer.train(10, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
