{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can simple copy the single page source codes and it'll work just fine, provided you have Jax/Flax/Optax correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from typing import Tuple, Any, Optional, Iterable\n",
    "\n",
    "\n",
    "class RotaryPositionalEncoding():\n",
    "    \"\"\"\n",
    "    The rotary position embeddings from RoFormer_ (Su et. al).\n",
    "    A crucial insight from the method is that the query and keys are\n",
    "    transformed by rotation matrices which depend on the relative positions.\n",
    "\n",
    "    Other implementations are available in the Rotary Transformer repo_ and in\n",
    "    GPT-NeoX_, GPT-NeoX was an inspiration\n",
    "\n",
    "    .. _RoFormer: https://arxiv.org/abs/2104.09864\n",
    "    .. _repo: https://github.com/ZhuiyiTechnology/roformer\n",
    "    .. _GPT-NeoX: https://github.com/EleutherAI/gpt-neox\n",
    "\n",
    "\n",
    "    .. This is implemented outside nn module as is modifies an external state\n",
    "       It is also puporsefully broken down for explainability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_model: The dimension of the input and output embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        inv_freq = 1.0 / (10000 ** (jnp.arange(0, dim_model, 2, dtype=jnp.float32) / dim_model))\n",
    "        self.inv_freq = inv_freq\n",
    "\n",
    "        self._seq_len_cached = None\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "\n",
    "    def _update_cos_sin_tables(self, x, seq_dimension=1):\n",
    "        \"\"\"\n",
    "        Update the cached cosine and sine tables, if necessary.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor, of shape `(batch_size, seq_len, dim)`.\n",
    "            seq_dimension: The dimension that represents the sequence length.\n",
    "\n",
    "        Returns:\n",
    "            The updated cosine and sine tables.\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[seq_dimension]\n",
    "\n",
    "        if seq_len != self._seq_len_cached:\n",
    "            self._seq_len_cached = seq_len\n",
    "            t = jnp.arange(seq_len, dtype=self.inv_freq.dtype)\n",
    "            freqs = jnp.outer(t, self.inv_freq)\n",
    "            emb = jnp.concatenate((freqs, freqs), axis=-1)\n",
    "            self._cos_cached = jnp.cos(emb)[None, None, :, :]\n",
    "            self._sin_cached = jnp.sin(emb)[None, None, :, :]\n",
    "\n",
    "        return self._cos_cached, self._sin_cached\n",
    "\n",
    "    def rotate_half(self, x):\n",
    "        \"\"\"\n",
    "        Split the input tensor into two halves, rotate the second half by 180 degrees, and concatenate the two halves back together.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor, of shape `(batch_size, seq_len, dim)`.\n",
    "\n",
    "        Returns:\n",
    "            The rotated tensor.\n",
    "        \"\"\"\n",
    "        x1, x2 = jnp.split(x, 2, axis=-1)\n",
    "        return jnp.concatenate((-x2, x1), axis=-1)\n",
    "\n",
    "    def apply_rotary_pos_emb(self, x, cos, sin):\n",
    "        \"\"\"\n",
    "         Apply the rotary position embeddings to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor, of shape `(batch_size, seq_len, dim)`.\n",
    "            cos: The cosine table, of shape `(batch_size, 1, seq_len, dim)`.\n",
    "            sin: The sine table, of shape `(batch_size, 1, seq_len, dim)`.\n",
    "\n",
    "        Returns:\n",
    "            The embedded tensor.\n",
    "        \"\"\"\n",
    "        cos = cos[:, :, : x.shape[-2], :]\n",
    "        sin = sin[:, :, : x.shape[-2], :]\n",
    "        return (x * cos) + (self.rotate_half(x) * sin)\n",
    "\n",
    "    def __call__(self, q, k):\n",
    "        \"\"\"\n",
    "         Apply the rotary position embeddings to the query and key tensors.\n",
    "\n",
    "        Args:\n",
    "            q: The query tensor, of shape `(batch_size, seq_len, dim)`.\n",
    "            k: The key tensor, of shape `(batch_size, seq_len, dim)`.\n",
    "\n",
    "        Returns:\n",
    "            The embedded query and key tensors.\n",
    "        \"\"\"\n",
    "        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n",
    "\n",
    "        return (\n",
    "            self.apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached)[0],\n",
    "            self.apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached)[0],\n",
    "        )\n",
    "    \n",
    "\n",
    "class GroupedRotaryShiftedWindowMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention which uses RoPE, Grouped Query and Sliding Window Attention.\n",
    "    \"\"\"\n",
    "    hidden_dim : int  # Output dimension\n",
    "    num_heads : int  # Number of parallel heads\n",
    "    num_groups : int  # Number of groups to split the heads into\n",
    "    window_size: int\n",
    "    shift_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.query_projection = nn.Dense(self.hidden_dim // self.num_heads,\n",
    "                                 kernel_init=nn.initializers.xavier_uniform(),\n",
    "                                 bias_init=nn.initializers.zeros,\n",
    "                                )\n",
    "        self.key_projection = nn.Dense(self.hidden_dim // (self.num_heads * self.num_groups),\n",
    "                                 kernel_init=nn.initializers.xavier_uniform(),\n",
    "                                 bias_init=nn.initializers.zeros \n",
    "                                )\n",
    "        self.value_projection = nn.Dense(self.hidden_dim // (self.num_heads * self.num_groups),\n",
    "                                 kernel_init=nn.initializers.xavier_uniform(),\n",
    "                                 bias_init=nn.initializers.zeros \n",
    "                                )\n",
    "        self.rope = RotaryPositionalEncoding(self.hidden_dim // self.num_groups)\n",
    "        self.output = nn.Dense(self.hidden_dim,\n",
    "                               kernel_init=nn.initializers.xavier_uniform(),\n",
    "                               bias_init=nn.initializers.zeros)\n",
    "\n",
    "    def __call__(self, \n",
    "                 inputs: jnp.ndarray, \n",
    "                 context: jnp.ndarray, \n",
    "                 mask: jnp.ndarray) -> tuple:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: inputs ((batch_size, seq_len, dims))\n",
    "            context: optional - context ((batch_size, seq_len, dims))\n",
    "            Mask: optional - masks where reqions to ignore are flipped to os\n",
    "                  regions to attend to are 1s (batch_size, seq_len, dims)\n",
    "\n",
    "        Return: outputs (batch_size, seq_len, seq_len)\n",
    "                attention matrixes (batch_size, heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        query = self.query_projection(inputs)\n",
    "        key = self.key_projection(context)\n",
    "        value = self.value_projection(context)\n",
    "        \n",
    "        # Break query into groups and transpose to (num_groups, batch_size, seq_len, dims)\n",
    "        # This will allow vmapping over the groups for parallelization\n",
    "        grouped_query = jnp.reshape(query, (query.shape[0], query.shape[1], self.num_groups, -1))\n",
    "        grouped_query = jnp.repeat(grouped_query, self.num_heads, axis=-1)\n",
    "        grouped_query = jnp.transpose(grouped_query, (2, 0, 1, 3))\n",
    "\n",
    "        # Repeat the key and values\n",
    "        key = jnp.repeat(key, self.num_heads, axis=-1)\n",
    "        value = jnp.repeat(value, self.num_heads, axis=-1)\n",
    "        \n",
    "        # Vectorize the process_group function\n",
    "        vectorized_process_group = jax.vmap(self.process_group, in_axes=(0, None, None, None))\n",
    "        results = vectorized_process_group(grouped_query, key, value, mask)\n",
    "\n",
    "        # Merge the groups back together\n",
    "        context_vectors = jnp.concatenate(results[0], axis=-1)\n",
    "        return self.output(context_vectors), results[1]\n",
    "    \n",
    "    def process_group(self, query, key, value, mask):\n",
    "        query, key = self.rope(query, key)\n",
    "        query_windows = self.window_partition(query)\n",
    "        key_windows = self.window_partition(key)\n",
    "        value_windows = self.window_partition(value)\n",
    "        attention_windows, attention_maps = self.attention_function(query_windows, \n",
    "                                                                    key_windows, \n",
    "                                                                    value_windows,\n",
    "                                                                    mask)\n",
    "\n",
    "        attention_windows = jnp.roll(attention_windows, -self.shift_size, axis=1)\n",
    "        merged = attention_windows.transpose((1, 0, 2, 3))\n",
    "        return jnp.reshape(merged, query.shape), attention_maps\n",
    "\n",
    "    def window_partition(self, x):\n",
    "        B, N, C = x.shape\n",
    "        assert N % self.window_size == 0, \"Sequence length must be a multiple of the window size\"\n",
    "        windows = jnp.reshape(x, (B, -1, self.window_size, C))  # (batch_size, num_windows, window_size, dim)\n",
    "        windows = windows.transpose((1, 0, 2, 3))  # Transpose to (num_windows, batch_size, window_size, dim)\n",
    "        return windows\n",
    "\n",
    "    def attention_function(self, query, key, value, mask):\n",
    "        input_length = query.shape[-2]\n",
    "        context_length = key.shape[-2]\n",
    "        head_dim = query.shape[-1] // self.num_heads\n",
    "        dim_key = key.shape[-1]\n",
    "\n",
    "        # Split keys, and values into heads\n",
    "        query_heads = jnp.reshape(query, (query.shape[0], query.shape[1], self.num_heads, input_length, head_dim))\n",
    "        key_heads = jnp.reshape(key, (key.shape[0], key.shape[1], self.num_heads, context_length, head_dim))\n",
    "        value_heads = jnp.reshape(value, (value.shape[0], value.shape[1], self.num_heads, context_length, head_dim))\n",
    "\n",
    "        attention_scores = jnp.matmul(query_heads, key_heads.transpose(0, 1, 2, 4, 3)) / jnp.sqrt(dim_key)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = self.causal_mask(attention_scores.shape)\n",
    "            attention_scores = attention_scores * mask\n",
    "\n",
    "        attention_weights = jax.nn.softmax(attention_scores, axis=-1)\n",
    "        attended_values = jnp.matmul(attention_weights, value_heads)\n",
    "        attended_values = attended_values.transpose(0, 1, 3, 2, 4)\n",
    "        attended_values = jnp.reshape(attended_values, (query.shape[0], query.shape[1], input_length, query.shape[-1]))\n",
    "        return attended_values, attention_weights\n",
    "    \n",
    "    def causal_mask(self, \n",
    "                shape: Tuple[int, ...]) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a causal mask for attention.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Batch size.\n",
    "            destination_dim (int): Dimension of the destination sequence.\n",
    "            source_dim (int): Dimension of the source sequence.\n",
    "\n",
    "        Returns:\n",
    "            jnp.ndarray: Causal mask with shape (batch_size, num_heads, destination_dim, source_dim).\n",
    "        \"\"\"\n",
    "        # Create index tensors for the source and destination dimensions\n",
    "        source_dim, destination_dim = shape[-2], shape[-2]\n",
    "        idx_source = jnp.arange(destination_dim)[:, None]\n",
    "        idx_destination = jnp.arange(source_dim)\n",
    "        mask = idx_source >= idx_destination - source_dim + destination_dim\n",
    "        mask = mask.astype(jnp.int32) \n",
    "\n",
    "        # Expand dimensions to match the required output shape\n",
    "        mask = mask[None, None, None, :, :]\n",
    "        return jnp.broadcast_to(mask, (shape[0], shape[1], shape[2], destination_dim, source_dim))\n",
    "    \n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network which incorporates SwiGLU activation.\n",
    "\n",
    "    Args:\n",
    "        num_hiddens (int): Number of hidden units in the feed-forward layers.\n",
    "        num_outputs (int): Number of output units in the feed-forward layers.\n",
    "    \"\"\"\n",
    "    hidden_dim: int\n",
    "    dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(self.hidden_dim, kernel_init=nn.initializers.xavier_uniform())\n",
    "        self.dense2 = nn.Dense(self.dim, kernel_init=nn.initializers.xavier_uniform())\n",
    "        self.dense3 = nn.Dense(self.hidden_dim, kernel_init=nn.initializers.xavier_uniform())\n",
    "\n",
    "    def __call__(self, X: jnp.ndarray) -> jnp.ndarray:\n",
    "        return self.dense2(nn.silu(self.dense1(X) * self.dense3(X)))\n",
    "    \n",
    "    \n",
    "class MistralDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder Block.\n",
    "\n",
    "    Args:\n",
    "        hidden_dim (int): Input dimension.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        feedforward_dim (int): Dimension of the feed-forward network.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    feedforward_dim: int\n",
    "    dropout: float\n",
    "    num_groups: int\n",
    "    window_size: int\n",
    "    shift_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention1 = GroupedRotaryShiftedWindowMultiHeadAttention(hidden_dim=self.hidden_dim, \n",
    "                                                          num_heads=self.num_heads,\n",
    "                                                          num_groups=self.num_groups,\n",
    "                                                          window_size=self.window_size,\n",
    "                                                          shift_size=self.shift_size)\n",
    "        \n",
    "        self.attention2 = GroupedRotaryShiftedWindowMultiHeadAttention(hidden_dim=self.hidden_dim, \n",
    "                                                          num_heads=self.num_heads,\n",
    "                                                          num_groups=self.num_groups,\n",
    "                                                          window_size=self.window_size,\n",
    "                                                          shift_size=self.shift_size)\n",
    "        \n",
    "        self.feed_forward = PositionWiseFFN(self.feedforward_dim, self.hidden_dim)\n",
    "        self.norm1 = nn.RMSNorm(self.dropout)\n",
    "        self.norm2 = nn.RMSNorm(self.dropout)\n",
    "        self.norm3 = nn.RMSNorm(self.dropout)\n",
    "        self.dropout1 = nn.Dropout(self.dropout)\n",
    "        self.dropout2 = nn.Dropout(self.dropout)\n",
    "        self.dropout3 = nn.Dropout(self.dropout)\n",
    "\n",
    "    def __call__(self, \n",
    "                x: jnp.ndarray,\n",
    "                training: bool = False) -> tuple:\n",
    "        \"\"\"\n",
    "        Apply the DecoderBlock to input data.\n",
    "\n",
    "        Args:\n",
    "            x (jnp.ndarray): Input tensor.\n",
    "            context (jnp.ndarray): Context tensor.\n",
    "            mask (jnp.ndarray, optional): Mask tensor. Defaults to None.\n",
    "            training (bool): Training mode.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Output tensor, attention tensor, and cross-attention tensor.\n",
    "        \"\"\"\n",
    "        x = self.norm1(x)\n",
    "        attended_x, attention1 = self.attention1(x, x, mask=True)\n",
    "        x = self.dropout1(x, deterministic=not training)\n",
    "        x += attended_x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        attended_x, attention2 = self.attention2(x, x, mask=True)\n",
    "        x = self.dropout2(x, deterministic=not training)\n",
    "        x += attended_x\n",
    "\n",
    "        x = self.norm3(x)\n",
    "        output = self.feed_forward(x)\n",
    "        x = self.dropout3(x, deterministic=not training)\n",
    "        x += output\n",
    "\n",
    "        return x, jnp.array(attention1), jnp.array(attention2)\n",
    "\n",
    "\n",
    "class MistralDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder.\n",
    "\n",
    "    Args:\n",
    "        num_layers (int): Number of decoder layers.\n",
    "        hidden_dim (int): Input dimension.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        feedforward_dim (int): Dimension of the feed-forward network.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "    num_layers: int\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    num_groups: int\n",
    "    feedforward_dim: int\n",
    "    dropout: float\n",
    "    vocab_size: float\n",
    "    embed_dim: float\n",
    "    window_size: int\n",
    "    shift_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, \n",
    "                                  features=self.embed_dim)\n",
    "        \n",
    "        self.layers = [MistralDecoderBlock(self.hidden_dim, \n",
    "                                    self.num_heads, \n",
    "                                    self.feedforward_dim, \n",
    "                                    self.dropout,\n",
    "                                    self.num_groups,\n",
    "                                    self.window_size,\n",
    "                                    self.shift_size) for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.outputs = nn.Dense(self.vocab_size)\n",
    "        \n",
    "\n",
    "    def __call__(self, \n",
    "                 x: jnp.ndarray,\n",
    "                 training: bool = False,\n",
    "                 drop_last_layer: bool = False) -> tuple:\n",
    "        \"\"\"\n",
    "        Apply the TransformerDecoder to input data.\n",
    "\n",
    "        Args:\n",
    "            x (jnp.ndarray): Input tensor.\n",
    "            context (jnp.ndarray): Context tensor.\n",
    "            mask (jnp.ndarray, optional): Mask tensor. Defaults to None.\n",
    "            training (bool): Training mode.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Output tensor, list of attention tensors, and list of cross-attention tensors.\n",
    "            each attention map has dim (num_layers, batch_size, num_heads, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "        attention_maps = []\n",
    "        x = self.embedding(x)\n",
    "        cross_attention_maps = []\n",
    "        for layer in self.layers:\n",
    "            x, attention, cross_attention = layer(x, training=training)\n",
    "            attention_maps.append(attention)\n",
    "            cross_attention_maps.append(cross_attention)\n",
    "\n",
    "        if not drop_last_layer:\n",
    "            x = self.outputs(x)\n",
    "\n",
    "        return x, jnp.array(attention_maps), jnp.array(cross_attention_maps)\n",
    "    \n",
    "\n",
    "\n",
    "class Mistral(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_layers (int): Number of layers in the encoder and decoder.\n",
    "        num_heads (int): Number of attention heads in the multi-head attention layers.\n",
    "        hidden_dim (int): Dimensionality of input embeddings.\n",
    "        feedforward_dim (int): Dimensionality of the feedforward layers.\n",
    "        dropout (float): Dropout probability.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimensionality of token embeddings.\n",
    "        max_length (int): Maximum length of generated sequences.\n",
    "        start_token (int): Token ID for the start of sequence.\n",
    "        end_token (int): Token ID for the end of sequence.\n",
    "    \"\"\"\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    num_groups: int\n",
    "    hidden_dim: int\n",
    "    feedforward_dim: int\n",
    "    dropout: float\n",
    "    vocab_size: float\n",
    "    embed_dim: float\n",
    "    max_length: int\n",
    "    start_token: int\n",
    "    end_token: int\n",
    "    window_size: int\n",
    "    shift_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Initialize the Mistral model \n",
    "        \"\"\"\n",
    "        self.decoder = MistralDecoder(self.num_layers,\n",
    "                                self.hidden_dim,\n",
    "                                self.num_heads,\n",
    "                                self.num_groups,\n",
    "                                self.feedforward_dim,\n",
    "                                self.dropout,\n",
    "                                self.vocab_size,\n",
    "                                self.embed_dim,\n",
    "                                self.window_size,\n",
    "                                self.shift_size)\n",
    "        \n",
    "    def __call__(self, \n",
    "                 x: jnp.ndarray,\n",
    "                 training: bool = False,\n",
    "                 drop_last_layer: bool = False) -> jnp.ndarray:\n",
    "        \n",
    "        \"\"\" \n",
    "        Sequence-to-sequence models use teacher forcing during training and as such, \n",
    "        the decoder input is the ground truth sequence.\n",
    "        \"\"\"\n",
    "        return self.decoder(x=x, \n",
    "                            training=training,\n",
    "                            drop_last_layer=drop_last_layer)[0]\n",
    "    \n",
    "    def zero_pad(self, arr, max_length):\n",
    "        \"\"\"Zero-pad the given array to the specified maximum length along axis=1.\"\"\"\n",
    "        current_length = arr.shape[1] \n",
    "        num_zeros = max_length - current_length \n",
    "\n",
    "        if num_zeros > 0:\n",
    "            zeros = jnp.zeros((arr.shape[0], num_zeros), dtype=arr.dtype)\n",
    "            padded_array = jnp.concatenate([arr, zeros], axis=1)\n",
    "        else:\n",
    "            padded_array = arr\n",
    "\n",
    "        return padded_array\n",
    "    \n",
    "\n",
    "    def generate(self, \n",
    "                 x: Optional[jnp.ndarray] = None,\n",
    "                 temperature: float = 1.0,\n",
    "                 deterministic: bool = False) -> Tuple[jnp.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate sequences either from scratch or continues from the input sequence.\n",
    "\n",
    "        Args:\n",
    "            x (jax.numpy.ndarray, optional): Input sequence.\n",
    "            temperature (float, optional): Temperature for token sampling. Higher values result in more randomness.\n",
    "            seed (int, optional): Random seed for reproducibility.\n",
    "            deterministic (bool, optional): If True, selects the most probable next word without random sampling.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[jax.numpy.ndarray]: A tuple containing the generated sequence.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            assert x.shape[0] == 1, \"Batch size must be 1, else use generate_batch()\"\n",
    "\n",
    "        decoder_input = x if x is not None else jnp.array([[self.start_token]])\n",
    "        output_sequence = []\n",
    "\n",
    "        # Autoregressive decoding loop\n",
    "        for _ in range(self.max_length - 1):\n",
    "            decoder_output = self.decoder(self.zero_pad(decoder_input, self.max_length), training=False)[0]\n",
    "            last_token_logits = decoder_output[:, -1, :]\n",
    "            scaled_logits = last_token_logits / temperature\n",
    "            next_token_probabilities = jax.nn.softmax(scaled_logits, axis=-1)\n",
    "\n",
    "            if deterministic:\n",
    "                next_token = jnp.argmax(next_token_probabilities, axis=-1)\n",
    "            else:\n",
    "                next_token = jax.random.categorical(jax.random.PRNGKey(int(time.time())), next_token_probabilities, axis=-1)\n",
    "\n",
    "            next_token = next_token[0]\n",
    "            output_sequence.append(next_token.item())\n",
    "            decoder_input = jnp.concatenate([decoder_input, jnp.array([[next_token]])], axis=1)\n",
    "\n",
    "            if next_token.item() == self.end_token:\n",
    "                break\n",
    "\n",
    "        return tuple(output_sequence)\n",
    "    \n",
    "\n",
    "    def generate_batch(self, \n",
    "                 x: Optional[jnp.ndarray] = None,\n",
    "                 temperature: float = 1.0,\n",
    "                 deterministic: bool = False) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Generate sequences either from scratch or continues from the input sequence in batch.\n",
    "\n",
    "        Args:\n",
    "            x (jax.numpy.ndarray, optional): Batch of input sequences.\n",
    "            temperature (float, optional): Temperature for token sampling. Higher values result in more randomness.\n",
    "            deterministic (bool, optional): If True, selects the most probable next word without random sampling.\n",
    "\n",
    "        Returns:\n",
    "            jax.numpy.ndarray: An array containing the generated sequences for each sample in the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = x.shape[0] if x is not None else 1\n",
    "        decoder_input = x if x is not None else jnp.full((batch_size, 1), self.start_token)\n",
    "        output_sequences = jnp.zeros((batch_size, self.max_length), dtype=jnp.int32)\n",
    "\n",
    "        for i in range(self.max_length-1):\n",
    "            decoder_output = self.decoder(self.zero_pad(decoder_input, self.max_length), training=False)[0]\n",
    "            last_token_logits = decoder_output[:, -1, :]\n",
    "            scaled_logits = last_token_logits / temperature\n",
    "            next_token_probabilities = jax.nn.softmax(scaled_logits, axis=-1)\n",
    "\n",
    "            if deterministic:\n",
    "                next_token = jnp.argmax(next_token_probabilities, axis=-1)\n",
    "            else:\n",
    "                key = jax.random.PRNGKey(int(time.time()))\n",
    "                next_token = jax.random.categorical(key, next_token_probabilities, axis=-1)\n",
    "\n",
    "            output_sequences = output_sequences.at[:, i].set(next_token)\n",
    "            decoder_input = jnp.concatenate([decoder_input, next_token[:, None]], axis=1)\n",
    "\n",
    "            if jnp.all(next_token == self.end_token):\n",
    "                break\n",
    "\n",
    "        return output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 50) (8, 50)\n",
      "(8, 50, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Dummy data parameters\n",
    "batch_size = 8\n",
    "max_length = 50\n",
    "vocab_size = 1000 \n",
    "embed_dim = 256 \n",
    "\n",
    "# Generate data\n",
    "data = jnp.arange(batch_size * (max_length+1), dtype=jnp.int32).reshape((batch_size, max_length+1))\n",
    "dummy_inputs = data[:, :-1]\n",
    "dummy_targets = data[:, 1:]\n",
    "print(dummy_inputs.shape, dummy_targets.shape)\n",
    "\n",
    "# model parameters\n",
    "hyperparams = {\n",
    "    'num_layers': 1,\n",
    "    'num_groups': 2,\n",
    "    'hidden_dim': 256,\n",
    "    'num_heads': 2,\n",
    "    'feedforward_dim': 256,\n",
    "    'dropout': 0.1,\n",
    "    'vocab_size': 1000,\n",
    "    'embed_dim': 256,\n",
    "    'max_length': max_length,\n",
    "    'start_token': 0,\n",
    "    'end_token': 50,\n",
    "    'window_size': 5,\n",
    "    'shift_size': 2\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = Mistral(**hyperparams)\n",
    "rngs = {'params': jax.random.key(0), 'dropout': jax.random.key(1)}\n",
    "params = model.init(rngs, dummy_inputs)['params']\n",
    "outputs = model.apply({'params': params}, dummy_inputs, rngs={'dropout': jax.random.PRNGKey(2)})\n",
    "print(outputs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
